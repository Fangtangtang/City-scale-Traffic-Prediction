{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# code from https://github.com/ts-kim/RevIN, with minor modifications\n# 在时序数据上平稳输入的分布（类似于一个对时序数据的编码和解码）\n\nimport torch\nimport torch.nn as nn\n\nclass RevIN(nn.Module):\n    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n        \"\"\"\n        :param num_features: the number of features or channels\n        :param eps: a value added for numerical stability\n        :param affine: if True, RevIN has learnable affine parameters\n        \"\"\"\n        super(RevIN, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.subtract_last = subtract_last\n        if self.affine:\n            self._init_params()\n\n    def forward(self, x, mode:str):\n        if mode == 'norm':\n            self._get_statistics(x)\n            x = self._normalize(x)\n        elif mode == 'denorm':\n            x = self._denormalize(x)\n        else: raise NotImplementedError\n        return x\n\n    def _init_params(self):\n        # initialize RevIN params: (C,)\n        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n\n    def _get_statistics(self, x):\n        dim2reduce = tuple(range(1, x.ndim-1))\n        if self.subtract_last:\n            self.last = x[:,-1,:].unsqueeze(1)\n        else:\n            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n\n    def _normalize(self, x):\n        if self.subtract_last:\n            x = x - self.last\n        else:\n            x = x - self.mean\n        x = x / self.stdev\n        if self.affine:\n            x = x * self.affine_weight\n            x = x + self.affine_bias\n        return x\n\n    def _denormalize(self, x):\n        if self.affine:\n            x = x - self.affine_bias\n            x = x / (self.affine_weight + self.eps*self.eps)\n        x = x * self.stdev\n        if self.subtract_last:\n            x = x + self.last\n        else:\n            x = x + self.mean\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:32:08.033402Z","iopub.execute_input":"2024-05-18T10:32:08.033930Z","iopub.status.idle":"2024-05-18T10:32:08.053653Z","shell.execute_reply.started":"2024-05-18T10:32:08.033892Z","shell.execute_reply":"2024-05-18T10:32:08.051975Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport math\n\n# Some Useful Tools\n#########################################################################################################\n\n#########################################################################################################\nclass Transpose(nn.Module):\n    def __init__(self, *dims, contiguous=False): \n        super().__init__()\n        self.dims, self.contiguous = dims, contiguous\n    def forward(self, x):\n        if self.contiguous: return x.transpose(*self.dims).contiguous()\n        else: return x.transpose(*self.dims)\n\n    \n#########################################################################################################\ndef get_activation_fn(activation):\n    if callable(activation): return activation()\n    elif activation.lower() == \"relu\": return nn.ReLU()\n    elif activation.lower() == \"gelu\": return nn.GELU()\n    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable') \n    \n    \n# decomposition\n#########################################################################################################\nclass moving_avg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(moving_avg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x):\n        # padding on the both ends of time series\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        x = x.permute(0, 2, 1)\n        return x\n\n\nclass series_decomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp, self).__init__()\n        self.moving_avg = moving_avg(kernel_size, stride=1)\n\n    def forward(self, x):\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n    \n    \n    \n# pos_encoding\n#########################################################################################################\ndef PositionalEncoding(q_len, d_model, normalize=True):\n    pe = torch.zeros(q_len, d_model)\n    position = torch.arange(0, q_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    if normalize:\n        pe = pe - pe.mean()\n        pe = pe / (pe.std() * 10)\n    return pe\n\nSinCosPosEncoding = PositionalEncoding\n\ndef Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n    x = .5 if exponential else 1\n    i = 0\n    for i in range(100):\n        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n        if abs(cpe.mean()) <= eps: break\n        elif cpe.mean() > eps: x += .001\n        else: x -= .001\n        i += 1\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef positional_encoding(pe, learn_pe, q_len, d_model):\n    # Positional encoding\n    if pe == None:\n        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        learn_pe = False\n    elif pe == 'zero':\n        W_pos = torch.empty((q_len, 1))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'zeros':\n        W_pos = torch.empty((q_len, d_model))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'normal' or pe == 'gauss':\n        W_pos = torch.zeros((q_len, 1))\n        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n    elif pe == 'uniform':\n        W_pos = torch.zeros((q_len, 1))\n        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n    return nn.Parameter(W_pos, requires_grad=learn_pe)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:32:08.057635Z","iopub.execute_input":"2024-05-18T10:32:08.058106Z","iopub.status.idle":"2024-05-18T10:32:08.106036Z","shell.execute_reply.started":"2024-05-18T10:32:08.058068Z","shell.execute_reply":"2024-05-18T10:32:08.104345Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n\nfrom typing import Callable, Optional\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nimport torch.nn.functional as F\nimport numpy as np\n\n# from PatchTST.supervised.layers.PatchTST_layers import *\n# from PatchTST.supervised.layers.RevIN import RevIN\n\n\nclass PatchTST_backbone(nn.Module):\n    def __init__(\n        self,\n        encoder_input_size,  # channels: 1 in our case\n        # patching\n        patch_len: int,  # P\n        stride: int,  # S\n        context_window: int,  # L\n        target_window: int,  # T\n        padding_patch=None,\n        # normalization\n        use_RevIN=True,  # use RevIN to normalize\n        affine=True,  # learnable affine parameters?\n        subtract_last=False,  #\n        # Projection\n        dropout: float = 0.0,\n        pe: str = \"zeros\",\n        learn_pe: bool = True,\n        # transformer\n        n_layers: int = 3,\n        d_model=128,\n        n_heads=16,\n        d_k: Optional[int] = None,\n        d_v: Optional[int] = None,\n        d_ff: int = 256,\n        norm: str = \"BatchNorm\",\n        attn_dropout: float = 0.0,\n        pre_norm: bool = False,\n        act: str = \"gelu\",\n        res_attention: bool = True,\n        store_attn: bool = False,\n        # head\n        individual=True,  # individual layers for each var?\n        head_dropout=0,\n    ):\n\n        super().__init__()\n\n        # [Input]: encoder_input_size time series of context_window=L\n        # ------------------------------------------------------------------------------\n        # Norm\n        self.RevIN = use_RevIN\n        if self.RevIN:\n            self.RevIN_layer = RevIN(\n                encoder_input_size, affine=affine, subtract_last=subtract_last\n            )\n\n        # Patching\n        self.patch_len = patch_len\n        self.stride = stride\n        self.padding_patch = padding_patch\n        patch_num = int((context_window - patch_len) / stride + 1)\n        if padding_patch == \"end\":  # can be modified to general case\n            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n            patch_num += 1\n        # ------------------------------------------------------------------------------\n        # [Output]: patch_num=N patched time series of patch_len=P\n\n        # [Input]: patch_num=N patched time series of patch_len=P\n        # ------------------------------------------------------------------------------\n        # transformer-based components\n        # Projection + Position Embedding (convert size to D)\n        self.patch_num = patch_num\n        self.patch_len = patch_len\n        q_len = patch_num\n        self.W_P = nn.Linear(patch_len, d_model)\n        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n        # Residual dropout\n        self.dropout = nn.Dropout(dropout)\n\n        self.backbone = TSTEncoder(\n            q_len,\n            d_model,\n            n_heads,\n            d_k=d_k,\n            d_v=d_v,\n            d_ff=d_ff,\n            norm=norm,\n            attn_dropout=attn_dropout,\n            dropout=dropout,\n            pre_norm=pre_norm,\n            activation=act,\n            res_attention=res_attention,\n            n_layers=n_layers,\n            store_attn=store_attn,\n        )\n\n        # ------------------------------------------------------------------------------\n        # [Output]:\n\n        # [Input]:\n        # ------------------------------------------------------------------------------\n        # Flatten + Linear Head\n        self.head = Flatten_Head(\n            individual,\n            encoder_input_size,\n            d_model * patch_num,\n            target_window,  # output length: T\n            head_dropout=head_dropout,\n        )\n        # ------------------------------------------------------------------------------\n        # [Output]: predicted time series of target_window=L\n\n    def forward(self, z):\n        # ------------------------------------------------------------------------------\n        # Norm\n        if self.RevIN:\n            z = z.permute(0, 2, 1)\n            z = self.RevIN_layer(z, \"norm\")\n            z = z.permute(0, 2, 1)\n\n        # Patching\n        if self.padding_patch == \"end\":\n            z = self.padding_patch_layer(z)\n        z = z.unfold(\n            dimension=-1, size=self.patch_len, step=self.stride\n        )  # z: [bs x nvars x patch_num x patch_len]\n        z = z.permute(0, 1, 3, 2)  # z: [bs x nvars x patch_len x patch_num]\n        # ------------------------------------------------------------------------------\n\n        # ------------------------------------------------------------------------------\n        # model\n        n_vars = z.shape[1]\n        # Input encoding\n        z = z.permute(0, 1, 3, 2)  # z: [bs x nvars x patch_num x patch_len]\n        z = self.W_P(z)  # z: [bs x nvars x patch_num x d_model]\n        u = torch.reshape(\n            z, (z.shape[0] * z.shape[1], z.shape[2], z.shape[3])\n        )  # u: [bs * nvars x patch_num x d_model]\n        u = self.dropout(u + self.W_pos)  # u: [bs * nvars x patch_num x d_model]\n        # ------------------------------------------------------------------------------\n\n        # ------------------------------------------------------------------------------\n        # n transformers\n        # z: [bs * nvars x patch_num x d_model]\n        z = self.backbone(u)\n        # ------------------------------------------------------------------------------\n\n        # ------------------------------------------------------------------------------\n        z = torch.reshape(\n            z, (-1, n_vars, z.shape[-2], z.shape[-1])\n        )  # z: [bs x nvars x patch_num x d_model]\n        z = z.permute(0, 1, 3, 2)  # z: [bs x nvars x d_model x patch_num]\n\n        # ------------------------------------------------------------------------------\n        # Flatten + Linear Head\n        z = self.head(z)  # z: [bs x nvars x target_window]\n\n        # denorm\n        if self.RevIN:\n            z = z.permute(0, 2, 1)\n            z = self.RevIN_layer(z, \"denorm\")\n            z = z.permute(0, 2, 1)\n        return z\n\n\n# some `private` class defined here\n#########################################################################################################\n# Flatten\nclass Flatten_Head(nn.Module):\n    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n        super().__init__()\n\n        self.individual = individual\n        self.n_vars = n_vars\n\n        if self.individual:\n            self.linears = nn.ModuleList()\n            self.dropouts = nn.ModuleList()\n            self.flattens = nn.ModuleList()\n            for i in range(self.n_vars):\n                self.flattens.append(nn.Flatten(start_dim=-2))\n                self.linears.append(nn.Linear(nf, target_window))\n                self.dropouts.append(nn.Dropout(head_dropout))\n        else:\n            self.flatten = nn.Flatten(start_dim=-2)\n            self.linear = nn.Linear(nf, target_window)\n            self.dropout = nn.Dropout(head_dropout)\n\n    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n        if self.individual:\n            x_out = []\n            for i in range(self.n_vars):\n                z = self.flattens[i](x[:, i, :, :])  # z: [bs x d_model * patch_num]\n                z = self.linears[i](z)  # z: [bs x target_window]\n                z = self.dropouts[i](z)\n                x_out.append(z)\n            x = torch.stack(x_out, dim=1)  # x: [bs x nvars x target_window]\n        else:\n            x = self.flatten(x)\n            x = self.linear(x)\n            x = self.dropout(x)\n        return x\n\n\n# n层 Transformer Encoder\nclass TSTEncoder(nn.Module):\n    def __init__(\n        self,\n        q_len,\n        d_model,\n        n_heads,\n        d_k=None,\n        d_v=None,\n        d_ff=None,\n        norm=\"BatchNorm\",\n        attn_dropout=0.0,\n        dropout=0.0,\n        activation=\"gelu\",\n        res_attention=False,\n        n_layers=1,\n        pre_norm=False,\n        store_attn=False,\n    ):\n        super().__init__()\n\n        # 堆n_layers层\n        self.layers = nn.ModuleList(\n            [\n                TSTEncoderLayer(\n                    q_len,\n                    d_model,\n                    n_heads=n_heads,\n                    d_k=d_k,\n                    d_v=d_v,\n                    d_ff=d_ff,\n                    norm=norm,\n                    attn_dropout=attn_dropout,\n                    dropout=dropout,\n                    activation=activation,\n                    res_attention=res_attention,\n                    pre_norm=pre_norm,\n                    store_attn=store_attn,\n                )\n                for i in range(n_layers)\n            ]\n        )\n        self.res_attention = res_attention\n\n    def forward(\n        self,\n        src: Tensor,\n        key_padding_mask: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ):\n        output = src\n        scores = None\n        if self.res_attention:\n            for mod in self.layers:\n                output, scores = mod(\n                    output,\n                    prev=scores,\n                    key_padding_mask=key_padding_mask,\n                    attn_mask=attn_mask,\n                )\n            return output\n        else:\n            for mod in self.layers:\n                output = mod(\n                    output, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n                )\n            return output\n\n\n# 单个Transformer Encoder Block\nclass TSTEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        q_len,\n        d_model,\n        n_heads,\n        d_k=None,\n        d_v=None,\n        d_ff=256,\n        store_attn=False,\n        norm=\"BatchNorm\",\n        attn_dropout=0,\n        dropout=0.0,\n        bias=True,\n        activation=\"gelu\",\n        res_attention=False,\n        pre_norm=False,\n    ):\n        super().__init__()\n        assert (\n            not d_model % n_heads\n        ), f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n        d_k = d_model // n_heads if d_k is None else d_k\n        d_v = d_model // n_heads if d_v is None else d_v\n\n        # Multi-Head attention\n        self.res_attention = res_attention\n        self.self_attn = _MultiheadAttention(\n            d_model,\n            n_heads,\n            d_k,\n            d_v,\n            attn_dropout=attn_dropout,\n            proj_dropout=dropout,\n            res_attention=res_attention,\n        )\n\n        # Add & Norm\n        self.dropout_attn = nn.Dropout(dropout)\n        if \"batch\" in norm.lower():\n            self.norm_attn = nn.Sequential(\n                Transpose(1, 2), nn.BatchNorm1d(d_model), Transpose(1, 2)\n            )\n        else:\n            self.norm_attn = nn.LayerNorm(d_model)\n\n        # Position-wise Feed-Forward\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff, bias=bias),\n            get_activation_fn(activation),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model, bias=bias),\n        )\n\n        # Add & Norm\n        self.dropout_ffn = nn.Dropout(dropout)\n        if \"batch\" in norm.lower():\n            self.norm_ffn = nn.Sequential(\n                Transpose(1, 2), nn.BatchNorm1d(d_model), Transpose(1, 2)\n            )\n        else:\n            self.norm_ffn = nn.LayerNorm(d_model)\n\n        self.pre_norm = pre_norm\n        self.store_attn = store_attn\n\n    def forward(\n        self,\n        src: Tensor,\n        prev: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ) -> Tensor:\n\n        # Multi-Head attention sublayer\n        if self.pre_norm:\n            src = self.norm_attn(src)\n        ## Multi-Head attention\n        if self.res_attention:\n            src2, attn, scores = self.self_attn(\n                src,\n                src,\n                src,\n                prev,\n                key_padding_mask=key_padding_mask,\n                attn_mask=attn_mask,\n            )\n        else:\n            src2, attn = self.self_attn(\n                src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n            )\n        if self.store_attn:\n            self.attn = attn\n        ## Add & Norm\n        src = src + self.dropout_attn(\n            src2\n        )  # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_attn(src)\n\n        # Feed-forward sublayer\n        if self.pre_norm:\n            src = self.norm_ffn(src)\n        ## Position-wise Feed-Forward\n        src2 = self.ff(src)\n        ## Add & Norm\n        src = src + self.dropout_ffn(\n            src2\n        )  # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_ffn(src)\n\n        if self.res_attention:\n            return src, scores\n        else:\n            return src\n\n\nclass _MultiheadAttention(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        n_heads,\n        d_k=None,\n        d_v=None,\n        res_attention=False,\n        attn_dropout=0.0,\n        proj_dropout=0.0,\n        qkv_bias=True,\n        lsa=False,\n    ):\n        \"\"\"Multi Head Attention Layer\n        Input shape:\n            Q:       [batch_size (bs) x max_q_len x d_model]\n            K, V:    [batch_size (bs) x q_len x d_model]\n            mask:    [q_len x q_len]\n        \"\"\"\n        super().__init__()\n        d_k = d_model // n_heads if d_k is None else d_k\n        d_v = d_model // n_heads if d_v is None else d_v\n\n        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n\n        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n\n        # Scaled Dot-Product Attention (multiple heads)\n        self.res_attention = res_attention\n        self.sdp_attn = _ScaledDotProductAttention(\n            d_model,\n            n_heads,\n            attn_dropout=attn_dropout,\n            res_attention=self.res_attention,\n            lsa=lsa,\n        )\n\n        # Project output\n        self.to_out = nn.Sequential(\n            nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout)\n        )\n\n    def forward(\n        self,\n        Q: Tensor,\n        K: Optional[Tensor] = None,\n        V: Optional[Tensor] = None,\n        prev: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ):\n\n        bs = Q.size(0)\n        if K is None:\n            K = Q\n        if V is None:\n            V = Q\n\n        # Linear (+ split in multiple heads)\n        q_s = (\n            self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n        )  # q_s    : [bs x n_heads x max_q_len x d_k]\n        k_s = (\n            self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0, 2, 3, 1)\n        )  # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n        v_s = (\n            self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1, 2)\n        )  # v_s    : [bs x n_heads x q_len x d_v]\n\n        # Apply Scaled Dot-Product Attention (multiple heads)\n        if self.res_attention:\n            output, attn_weights, attn_scores = self.sdp_attn(\n                q_s,\n                k_s,\n                v_s,\n                prev=prev,\n                key_padding_mask=key_padding_mask,\n                attn_mask=attn_mask,\n            )\n        else:\n            output, attn_weights = self.sdp_attn(\n                q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n            )\n        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n\n        # back to the original inputs dimensions\n        output = (\n            output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v)\n        )  # output: [bs x q_len x n_heads * d_v]\n        output = self.to_out(output)\n\n        if self.res_attention:\n            return output, attn_weights, attn_scores\n        else:\n            return output, attn_weights\n\n\nclass _ScaledDotProductAttention(nn.Module):\n    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n    by Lee et al, 2021)\"\"\"\n\n    def __init__(\n        self, d_model, n_heads, attn_dropout=0.0, res_attention=False, lsa=False\n    ):\n        super().__init__()\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.res_attention = res_attention\n        head_dim = d_model // n_heads\n        self.scale = nn.Parameter(torch.tensor(head_dim**-0.5), requires_grad=lsa)\n        self.lsa = lsa\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        prev: Optional[Tensor] = None,\n        key_padding_mask: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n    ):\n        \"\"\"\n        Input shape:\n            q               : [bs x n_heads x max_q_len x d_k]\n            k               : [bs x n_heads x d_k x seq_len]\n            v               : [bs x n_heads x seq_len x d_v]\n            prev            : [bs x n_heads x q_len x seq_len]\n            key_padding_mask: [bs x seq_len]\n            attn_mask       : [1 x seq_len x seq_len]\n        Output shape:\n            output:  [bs x n_heads x q_len x d_v]\n            attn   : [bs x n_heads x q_len x seq_len]\n            scores : [bs x n_heads x q_len x seq_len]\n        \"\"\"\n\n        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n        attn_scores = (\n            torch.matmul(q, k) * self.scale\n        )  # attn_scores : [bs x n_heads x max_q_len x q_len]\n\n        # Add pre-softmax attention scores from the previous layer (optional)\n        if prev is not None:\n            attn_scores = attn_scores + prev\n\n        # Attention mask (optional)\n        if (\n            attn_mask is not None\n        ):  # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n            if attn_mask.dtype == torch.bool:\n                attn_scores.masked_fill_(attn_mask, -np.inf)\n            else:\n                attn_scores += attn_mask\n\n        # Key padding mask (optional)\n        if (\n            key_padding_mask is not None\n        ):  # mask with shape [bs x q_len] (only when max_w_len == q_len)\n            attn_scores.masked_fill_(\n                key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf\n            )\n\n        # normalize the attention weights\n        attn_weights = F.softmax(\n            attn_scores, dim=-1\n        )  # attn_weights   : [bs x n_heads x max_q_len x q_len]\n        attn_weights = self.attn_dropout(attn_weights)\n\n        # compute the new values given the attention weights\n        output = torch.matmul(\n            attn_weights, v\n        )  # output: [bs x n_heads x max_q_len x d_v]\n\n        if self.res_attention:\n            return output, attn_weights, attn_scores\n        else:\n            return output, attn_weights\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:32:08.110207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torch import nn\nfrom torch import Tensor\nfrom typing import Callable, Optional\n\n# from PatchTST.supervised.layers.PatchTST_backbone import PatchTST_backbone\n# from PatchTST.supervised.layers.PatchTST_layers import series_decomp\n\nclass Model(nn.Module):\n\n    def __init__(\n        self,\n        decomposition,\n        encoder_input_size,  # channels: 1 in our case\n        # patching\n        patch_len: int,  # P\n        stride: int,  # S\n        context_window: int,  # L\n        target_window: int,  # T\n        padding_patch=None,\n        # normalization\n        use_RevIN=True,  # use RevIN to normalize\n        affine=True,  # learnable affine parameters?\n        subtract_last=False,  #\n        # Projection\n        dropout: float = 0.0,\n        pe: str = \"zeros\",\n        learn_pe: bool = True,\n        # transformer\n        n_layers: int = 3,\n        d_model=128,\n        n_heads=16,\n        d_k: Optional[int] = None,\n        d_v: Optional[int] = None,\n        d_ff: int = 256,\n        norm: str = \"BatchNorm\",\n        attn_dropout: float = 0.0,\n        pre_norm: bool = False,\n        act: str = \"gelu\",\n        res_attention: bool = True,\n        store_attn: bool = False,\n        # head\n        individual=True,  # individual layers for each var?\n        head_dropout=0,\n    ):\n        super().__init__()\n\n        self.decomposition = decomposition\n\n        if self.decomposition:\n            # ! todo\n            pass\n        else:\n            self.model = PatchTST_backbone(\n                encoder_input_size=encoder_input_size,\n                patch_len=patch_len,\n                stride=stride,\n                context_window=context_window,\n                target_window=target_window,\n                use_RevIN=use_RevIN,\n                affine=affine,\n                dropout=dropout,\n                n_layers=n_layers,\n                d_model=d_model,\n                n_heads=n_heads,\n                d_ff=d_ff,\n                norm=norm,\n                attn_dropout=attn_dropout,\n                pre_norm=pre_norm,\n                act=act,\n                res_attention=res_attention,\n                store_attn=store_attn,\n                individual=individual,\n                head_dropout=head_dropout,\n            )\n\n    def forward(self, x):  # x: [Batch, Input length, Channel]\n        if self.decomposition:\n            # ! todo\n            pass\n        else:\n            x = x.permute(0, 2, 1)  # x: [Batch, Channel, Input length]\n            x = self.model(x)\n            x = x.permute(0, 2, 1)  # x: [Batch, Input length, Channel]\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch import optim\n# import PatchTST.supervised.PatchTST_model as PatchTST_model\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nimport json\nfrom collections import defaultdict\n\n\nclass Exp(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = self._acquire_device()\n        self.channel_list = self.args.sensor_id\n        self.data_set = DataSet(\n            self.args.sensor_id,\n            size=[self.args.seq_len, self.args.label_len, self.args.pred_len],\n        )\n\n        self.args.context_window = len(self.data_set)\n\n        self.model = self._build_model().to(self.device)\n\n        print(len(self.data_set))\n        self.data_loader = DataLoader(\n            self.data_set, batch_size=self.args.batch_size, shuffle=True, drop_last=True\n        )\n\n    def _build_model(self):\n        model = PatchTST_model.Model(\n            decomposition=self.args.decomposition,\n            encoder_input_size=self.args.encoder_input_size,\n            patch_len=self.args.patch_len,\n            stride=self.args.stride,\n            context_window=self.args.context_window,\n            target_window=self.args.target_window,\n        )\n        if self.args.use_multi_gpu and self.args.use_gpu:\n            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n        return model\n\n    def _acquire_device(self):\n        if self.args.use_gpu:\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = (\n                str(self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n            )\n            device = torch.device(\"cuda:{}\".format(self.args.gpu))\n            print(\"Use GPU: cuda:{}\".format(self.args.gpu))\n        else:\n            device = torch.device(\"cpu\")\n            print(\"Use CPU\")\n        return device\n\n    def _get_data(self, flag):\n        return self.data_set, self.data_loader\n\n    def vali(self, vali_loader, criterion):\n        total_loss = []\n        self.model.eval()\n        # 不跟踪梯度，加快计算，减少内存消耗\n        with torch.no_grad():\n            for i, (batch_x, batch_y, stamp_x, stamp_y) in enumerate(vali_loader):\n                # put data to device\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float()\n\n                # apply\n                outputs = self.model(batch_x)\n\n                # get predicted feature of given length\n                f_dim = 0\n                outputs = outputs[:, -self.args.pred_len :, f_dim:]\n                batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)\n\n                pred = outputs.detach().cpu()\n                true = batch_y.detach().cpu()\n\n                # calculate loss\n                loss = criterion(pred, true)\n                total_loss.append(loss)\n\n        total_loss = np.average(total_loss)\n        # switch back to train mode\n        self.model.train()\n        return total_loss\n\n    def train(self, setting):\n        train_data, train_loader = self._get_data(flag=\"train\")\n\n        path = os.path.join(self.args.checkpoints, setting)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        criterion = nn.MSELoss()\n\n        loss_list = []\n        for epoch in range(self.args.train_epochs):\n            if epoch % 10 == 0:\n                print(\n                    \">>>>>>>>>>>>>>>>>>>> Epoch {} <<<<<<<<<<<<<<<<<<<<<\".format(epoch)\n                )\n\n            iter_count = 0\n            train_loss = []\n\n            self.model.train()\n            for i, (batch_x, batch_y, stamp_x, stamp_y) in enumerate(train_loader):\n                iter_count += 1\n                model_optim.zero_grad()\n              \n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float().to(self.device)\n\n                outputs = self.model(batch_x)\n\n                f_dim = 0\n                outputs = outputs[:, -self.args.pred_len :, f_dim:]\n                batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)\n              \n\n                loss = criterion(outputs, batch_y)\n                train_loss.append(loss.item())\n\n                loss.backward()\n                model_optim.step()\n\n            print(np.average(train_loss))\n            loss_list.append(np.average(train_loss))\n\n            if epoch % 10 == 0:\n                loss_list.append(np.average(train_loss))\n\n            if epoch > 0 and epoch % 50 == 0:\n                torch.save(\n                    self.model.state_dict(),\n                    path + \"/\" + \"checkpoint{}.pth\".format(epoch / 100),\n                )\n\n        print(loss_list)\n        torch.save(self.model.state_dict(), path + \"/\" + \"checkpoint.pth\")\n        return self.model, loss_list\n\n    def test(self, setting, test_model_from_path=0):\n        test_data, test_loader = self._get_data(flag=\"test\")\n\n        if test_model_from_path:\n            print(\"loading model\")\n            self.model.load_state_dict(\n                torch.load(os.path.join(\"./checkpoints/\" + setting, \"checkpoint.pth\"))\n            )\n\n        preds = []\n        trues = []\n        inputx = []\n        folder_path = \"./test_results/\" + setting + \"/\"\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n\n        self.model.eval()\n        # 不跟踪梯度，加快计算，减少内存消耗\n        with torch.no_grad():\n            for i, (batch_x, batch_y, stamp_x, stamp_y) in enumerate(test_loader):\n                # put data to device\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float()\n\n                # apply\n                outputs = self.model(batch_x)\n\n                # get predicted feature of given length\n                f_dim = 0\n                outputs = outputs[:, -self.args.pred_len :, f_dim:]\n                batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)\n                outputs = outputs.detach().cpu().numpy()\n                batch_y = batch_y.detach().cpu().numpy()\n\n                pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n                true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n\n                preds.append(pred)\n                trues.append(true)\n                inputx.append(batch_x.detach().cpu().numpy())\n\n        preds = np.array(preds)\n        trues = np.array(trues)\n        inputx = np.array(inputx)\n\n        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n\n        # result save\n        folder_path = \"./results/\" + setting + \"/\"\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n\n        np.save(folder_path + \"pred.npy\", preds)\n\n        return\n\n    def predict(self, setting, load_model_from_path=0):\n        pred_data, pred_loader = self._get_data(flag=\"pred\")\n\n        self.answer_list = {}\n        ans = {}\n        cnt = {}\n        for idx in self.channel_list:\n            self.answer_list[idx] = []\n\n        for i in range(len(self.channel_list)):\n            ans[i] = defaultdict(float)\n            cnt[i] = defaultdict(int)\n\n        if load_model_from_path:\n            path = os.path.join(self.args.checkpoints, setting)\n            best_model_path = path + \"/\" + \"checkpoint.pth\"\n            self.model.load_state_dict(torch.load(best_model_path))\n\n        preds = []\n\n        self.model.eval()\n        with torch.no_grad():\n            for i, (batch_x, batch_y, stamp_x, stamp_y) in enumerate(pred_loader):\n                # put data to device\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float()\n\n                # apply\n                outputs = self.model(batch_x)\n\n                # get predicted feature of given length\n                f_dim = 0\n                outputs = outputs[:, -self.args.pred_len :, f_dim:]\n                batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)\n\n                pred = outputs.detach().cpu().numpy()  # .squeeze()\n\n            \n                for i in range(batch_y.shape[0]):\n                    for j in range(batch_y.shape[1]):\n                        for k in range(batch_y.shape[2]):\n                            # for channel in range(batch_y.shape[2]):\n                            cnt[k][f\"{stamp_y[i][j][0]}\"] += 1\n                            ans[k][f\"{stamp_y[i][j][0]}\"] += pred[i][j][k]\n\n                preds.append(pred)\n\n        preds = np.array(preds)\n        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n\n        for channel in range(len(self.channel_list)):\n            averages = {label: ans[channel][label] / cnt[channel][label] for label in ans[channel]}\n            self.answer_list[self.channel_list[channel]]=averages\n\n        # result save\n        folder_path = \"./results/\" + setting + \"/\"\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n\n        np.save(folder_path + \"real_prediction.npy\", preds)\n\n        return self.answer_list, pred_data\n\n\nclass DataSet(Dataset):\n\n    def __init__(self, sensor_id_list, flag=\"train\", size=None, features=\"S\"):\n\n        if size == None:\n            self.seq_len = 16\n            self.label_len = 8\n            self.pred_len = 4\n        else:\n            self.seq_len = size[0]\n            self.label_len = size[1]\n            self.pred_len = size[2]\n\n        data={}\n        \n        data_y = []\n        for idx in sensor_id_list:\n            path = \"/kaggle/input/traffic-flow-in-paris/train_with_time/{}_avg.jsonl\".format(idx)\n\n            # data_x = []\n            data_y = []\n            with open(path, \"r\") as f:\n                for line in f:\n                    sample = json.loads(line)\n                    # data_x.append(sample[\"time\"])\n                    data_y.append(sample[\"traffic_flow\"])\n\n            data[\"{}_flow\".format(idx)] =  data_y\n\n        self.raw_data=data\n\n        self.data_len=len(data_y)\n        # data[\"stamp\"] =  range(self.data_len)\n\n        df = pd.DataFrame(data)\n        # df.set_index('stamp', inplace=True)\n        # cols_data = df.columns[1:]\n        cols_data = df.columns\n        print(cols_data)\n        self.data = df[cols_data].values\n        data = {\n            \"time\": range(self.data_len),\n        }\n        df = pd.DataFrame(data)\n        cols_data = df.columns\n        print(cols_data)\n        self.stamp = df[cols_data].values\n\n    def __len__(self):\n        return self.data_len - self.seq_len - self.pred_len + 1\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n\n        \n        return (\n            self.data[s_begin:s_end],\n            self.data[r_begin:r_end],\n            self.stamp[s_begin:s_end],\n            self.stamp[r_begin:r_end],\n        )\n\n    def raw_data_length(self):\n        return self.data_len\n    \n    def get_raw(self):\n        return self.raw_data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy\nimport json\nimport torch\nfrom datetime import datetime, timedelta\nimport os\nfrom typing import Union\nimport pandas as pd\nimport jsonlines\n\nclass DateLoader:\n    def __init__(self) -> None:\n        self.train_dic_path = \"/kaggle/input/traffic-flow-in-paris/train_with_time\"\n    \n    def pre_train_load(self, train_path=\"\") -> None:    # 17855 0\n                                                        # 2030\n        if os.path.exists(\"/kaggle/input/test-sensor-list/pre_test.jsonl\"):\n            return\n        train_data = {}\n        max_hour = 0\n        min_hour = 1 << 32\n        with open(train_path, \"r\") as fin:\n            for line in fin.readlines()[1:]:\n                line = line.rstrip('\\n')\n                line = line.split(',')\n                if len(line) != 4:\n                    raise ValueError(\"Wrong dara format!\")\n                time_end = datetime.strptime(line[1], r\"%Y-%m-%d %H:%M:%S\")\n                time_beg = datetime(year=2022, month=1, day=1, hour=0, minute=0, second=0)\n                time_dif = time_end - time_beg\n                diff_hour = int(time_dif.total_seconds() / 3600)\n                max_hour = max(max_hour, diff_hour)\n                min_hour = min(min_hour, diff_hour)\n                idx = line[0]\n                if idx not in train_data:\n                    train_data[idx] = []\n                train_data[idx].append([diff_hour, eval(line[3]), eval(line[2])])\n        print(max_hour, min_hour)\n        with open(\"/kaggle/input/test-sensor-list/pre_test.jsonl\", \"w\") as fout:\n            for idx in train_data:\n                fout.write(json.dumps({idx : train_data[idx]}) + '\\n')\n    \n    def pre_test_load(self, test_path=\"\") -> None:  # 17832 8784\n                                                    # 1757\n        if os.path.exists(\"/kaggle/input/test-sensor-list/pre_test.jsonl\"):\n            return\n        test_data = {}\n        max_hour = 0\n        min_hour = 1 << 32\n        with open(test_path, \"r\") as fin:\n            for line in fin.readlines()[1:]:\n                line = line.rstrip('\\n')\n                line = line.split(',')\n                if len(line) != 4:\n                    raise ValueError(\"Wrong dara format!\")\n                time_end = datetime.strptime(line[2], r\"%Y-%m-%d %H:%M:%S\")\n                time_beg = datetime(year=2022, month=1, day=1, hour=0, minute=0, second=0)\n                time_dif = time_end - time_beg\n                diff_hour = int(time_dif.total_seconds() / 3600)\n                max_hour = max(max_hour, diff_hour)\n                min_hour = min(min_hour, diff_hour)\n                idx = line[1]\n                if idx not in test_data:\n                    test_data[idx] = []\n                test_data[idx].append([diff_hour, eval(line[3])])\n        print(max_hour, min_hour)\n        with open(\"/kaggle/input/test-sensor-list/pre_test.jsonl\", \"w\") as fout:\n            for idx in test_data:\n                fout.write(json.dumps({idx : test_data[idx]}) + '\\n')\n\n    def load_test(self, test_path=\"\") -> dict:\n        if not os.path.exists(test_path):\n            return None\n        ret = {}\n        with open(test_path) as fw:\n            for line in fw:\n                line = json.loads(line)\n                for k in line:\n                    ret[k] = line[k]\n        return ret\n    \n    def load_train(self, train_path=\"\", idx=[]) -> dict:\n        ret = {}\n        for i in idx:\n            t_path = os.path.join(train_path, f\"{i}.jsonl\")\n            if not os.path.exists(t_path):\n                return None\n            with open(t_path, \"r\") as fw:\n                for line in fw:\n                    line = json.loads(line)\n                    for k in line:\n                        ret[k] = line[k]\n        return ret\n    \n    def create_train_data_with_padding(self,org_path,padding_type,id):\n        time_beg = datetime(year=2022, month=1, day=1, hour=0, minute=0, second=0)\n        current_time=time_beg\n        t_path = os.path.join(self.train_dic_path, f\"{id}_{padding_type}.jsonl\")\n        \n        entry_size=7*24\n        flow_sum=[0] *entry_size\n        valid_cnt=[0] *entry_size\n\n        # avg in week\n        larger_flow_sum=[0]*24\n        larger_valid_cnt=[0] *24\n        avg_hourly_flow=[0]*24\n\n        if padding_type==\"avg\" or padding_type==\"avg2\":\n            with open(org_path, \"r\") as fin:\n                cnt=0\n                for line in fin:\n                    line = json.loads(line)\n                    time= datetime.strptime(line['time'], r\"%Y-%m-%d %H:%M:%S\")\n                    while current_time != time:\n                        current_time = current_time + timedelta(hours=1)\n                        cnt+=1\n\n                    entry_idx=cnt%entry_size\n                    flow_sum[entry_idx]+=line[\"traffic_flow\"]\n                    valid_cnt[entry_idx]+=1\n                    # print(line['time'],'\\t',cnt%24,'\\t',cnt%entry_size)\n                    current_time = current_time + timedelta(hours=1)\n                    cnt+=1\n            if padding_type==\"avg\":\n                for i in range(entry_size):\n                    larger_flow_sum[i%24]+=flow_sum[i]\n                    larger_valid_cnt[i%24]+=valid_cnt[i]\n                \n                total_avg=sum(larger_flow_sum)/sum(larger_valid_cnt)\n                for i in range(24):\n                    if larger_valid_cnt[i]!=0:\n                        avg_hourly_flow[i]=larger_flow_sum[i]/larger_valid_cnt[i]\n                    else:\n                        avg_hourly_flow[i]=total_avg\n    \n\n        current_time=time_beg\n        with open(org_path, \"r\") as fin:\n            cnt=0\n            with jsonlines.open(t_path,\"a\") as f_out:\n                for line in fin:\n                    line = json.loads(line)\n                    line[\"config\"] = 1\n                    time= datetime.strptime(line['time'], r\"%Y-%m-%d %H:%M:%S\")\n                    while current_time != time:\n                        dict={}\n                        if padding_type==\"zero\":\n                            dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":0.1, \"config\":0}\n                        if padding_type==\"bak\":\n                            dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":line[\"traffic_flow\"], \"config\":0}\n                        if padding_type==\"avg\":\n                            entry_idx=cnt%entry_size\n                            avg_flow=0\n                            if valid_cnt[entry_idx]!=0:\n                                avg_flow = flow_sum[entry_idx]/valid_cnt[entry_idx]\n                            else:\n                                print(current_time.strftime('%Y-%m-%d %H:%M:%S')+\"\\t24*7 failed.\")\n                                avg_flow = avg_hourly_flow[entry_idx%24]\n                            if avg_flow < 0.1:\n                                avg_flow = 0.1\n                            dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":avg_flow, \"config\":0}\n                        f_out.write(dict)\n                        current_time = current_time + timedelta(hours=1)\n                        cnt+=1\n                    if line['traffic_flow'] < 0.1:\n                        line['traffic_flow'] = 0.1\n                    f_out.write(line)\n                    # print(line['time'],'\\t',cnt%24,'\\t',cnt%entry_size)\n                    current_time = current_time + timedelta(hours=1)\n                    cnt+=1\n                endtime = datetime(year=2024,month=1,day=15,hour=0,minute=0,second=0)\n                while current_time != endtime:\n                    dict={}\n                    if padding_type==\"zero\":\n                        dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":0.1, \"config\":0}\n                    if padding_type==\"bak\":\n                        dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":line[\"traffic_flow\"], \"config\":0}\n                    if padding_type==\"avg\":\n                        entry_idx=cnt%entry_size\n                        avg_flow=0\n                        if valid_cnt[entry_idx]!=0:\n                            avg_flow = flow_sum[entry_idx]/valid_cnt[entry_idx]\n                        else:\n                            print(current_time.strftime('%Y-%m-%d %H:%M:%S')+\"\\t24*7 failed.\")\n                            avg_flow = avg_hourly_flow[entry_idx%24]\n                        if avg_flow < 0.1:\n                            avg_flow = 0.1\n                        dict={\"sensor_id\":f\"{id}\",\"time\":current_time.strftime('%Y-%m-%d %H:%M:%S'),\"condition\":0,\"traffic_flow\":avg_flow, \"config\":0}\n                    f_out.write(dict)\n                    current_time = current_time + timedelta(hours=1)\n                    cnt+=1\n            \n\n    def load_train_as_dataframes(self,padding_type=\"\", idx=[]) -> dict:\n        '''\n            padding_type:\n                - \"\"\n                - \"zero\": use 0 for padding\n                - \"bak\": use the first data after it for padding\n                - \"avg\": average from existing data (24*7 entries)\n        '''\n        ret = {}\n        for i in idx:\n            t_path = os.path.join(self.train_dic_path, f\"{i}_{padding_type}.jsonl\")\n\n            if not os.path.exists(t_path):\n                self.create_train_data_with_padding(\n                    os.path.join(self.train_dic_path, f\"{i}_origin.jsonl\"),\n                    padding_type,\n                    id=i\n                )\n                \n            df = pd.read_json(path_or_buf=t_path, \n                                lines=True,\n                                convert_dates={'time': lambda x: pd.to_datetime(x, format='%Y-%m-%d %H', errors='coerce')}\n                                )\n            # 设置 'time' 列为索引， 以hour为粒度\n            begin_time = df.loc[0, \"time\"]\n            df.set_index('time', inplace=True)\n            df.index = pd.DatetimeIndex(df.index, freq='H')\n\n            ret[i]=[df, begin_time]\n\n        return ret\n\nMAX_ITERATION = 17855 - 0 + 1\n\ndef format_train_data(data:list) -> Union[int, torch.tensor, torch.tensor]:\n    ret = [[0.0, 0, 3] for _ in range(MAX_ITERATION)]\n    mem = [[0.0, 0] for _ in range(7*24)]\n    for line in data:\n        ret[line[0]][0], ret[line[0]][1], ret[line[0]][2] = line[1], 1, line[2]\n        mem[line[0] % (7*24)][0] = (mem[line[0] % (7*24)][0] * mem[line[0] % (7*24)][1] + line[1]) / (mem[line[0] % (7*24)][1] + 1)\n        mem[line[0] % (7*24)][1] += 1\n    for i in range(MAX_ITERATION):\n        line = ret[i]\n        if line[1] == 0:\n            line[0] = mem[i % (7*24)][0]\n    return data[0][0], torch.tensor(ret, dtype=torch.float32), torch.tensor(data)[:, 1]\n\nif __name__ == \"__main__\":\n    dl = DateLoader()\n    # dl.pre_test_load(test_path=\"data/pre_test.json\")\n    # dl.pre_train_load(train_path=\"data/pre_train.json\")\n    with open(\"data/pre_train.json\", \"r\") as fin:\n        for line in fin:\n            line = json.loads(line)\n            for k in line:\n                with open(f\"data/train/{k}.jsonl\", \"a\") as fout:\n                    fout.write(json.dumps(line) + '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport torch\nimport argparse\nimport jsonlines\nimport json\nimport matplotlib.pyplot as plt\n\n\n\n######################################################\nuse_gpu=True\n######################################################\n\ndata_loader = DateLoader()\ntest_data = data_loader.load_test(test_path=\"/kaggle/input/test-sensor-list\")\n\nwith open('/kaggle/input/worklist-for-sensor-prediction/work_list1.json', 'r') as file:\n    loaded_list = json.load(file)\nidx_list = list(loaded_list)\nprint(idx_list)\nans = {}\n\nargs = argparse.Namespace()\n\n# configs\nargs.use_gpu = True if torch.cuda.is_available() and use_gpu else False\nargs.use_multi_gpu = False\nargs.gpu = 0\nargs.checkpoints = \"./checkpoints/\"\n\n# training paras\nargs.train_epochs = 160\nargs.learning_rate = 0.0001\n\n# model super paras\nargs.decomposition = 0\nargs.encoder_input_size = 1\nargs.patch_len = 7 * 24\nargs.stride = 4 * 24\nargs.target_window = 24\nargs.padding_patch = \"end\"\nargs.batch_size = 64\nargs.seq_len = 7 * 24\nargs.label_len = 0\nargs.pred_len = 24\n\nfor idx in idx_list:\n\n    try:\n        print(idx)\n        ans[idx] = []\n        test_data_for_single = test_data[idx]\n\n        args.sensor_id = [str(idx)]\n        exp = Exp(args)  # set experiments\n\n        setting = \"setting_\" + args.sensor_id[0]\n\n        print(\">>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>\".format(setting))\n        model, loss_list  = exp.train(setting)\n        print(\">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting))\n        pred_result, data = exp.predict(setting, 0)\n        \n        raw_data = data.get_raw()[f\"{idx}_flow\"]\n        result_list = []\n        x_values = range(len(raw_data))\n\n\n        with jsonlines.open(\"/kaggle/working/loss_list.jsonl\", \"a\") as f_out:\n            dict={\n                \"id\":args.sensor_id,\n                \"loss_list\":loss_list,\n                }\n            f_out.write(dict)\n\n\n        result=pred_result[idx]\n        keys = list(result.keys())\n        values = list(result.values())\n\n        result_list = []\n        x_values = range(data.raw_data_length())\n\n        for i in x_values:\n            if f\"{i+1}\" in result:\n                result_list.append(result[f\"{i+1}\"])\n            else:\n                result_list.append(raw_data[i])\n\n        ans[idx] = []\n        test_data_for_single = test_data[idx]\n        for k in test_data_for_single:\n            ans[idx].append(result_list[k[0]].item())\n\n        with jsonlines.open(\"/kaggle/working/ans.json\", \"a\") as fout:\n            fout.write({idx:ans[idx]})\n\n    \n    except BaseException:\n        with open (\"/kaggle/working/error\",\"a\") as f:\n                f.write(idx+'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}